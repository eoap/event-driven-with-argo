{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Event-Driven Water Bodies Detection Using Argo Workflows and Argo Events","text":""},{"location":"#introduction","title":"Introduction","text":"<p>This learning resource demonstrates an event-driven system for detecting water bodies using cloud-native technologies. The system leverages Argo Events to handle and react to external event sources and Argo Workflows to execute data processing pipelines, including a water bodies detection algorithm encoded in the Common Workflow Language (CWL).</p> <p>The workflow is triggered by events simulated through Redis, an external event source, which queries a SpatioTemporal Asset Catalog (STAC) endpoint. The STAC endpoint provides geospatial data, which serves as the input for the detection algorithm. The automation is achieved using Kubernetes-native tools, making the setup scalable, modular, and suitable for Earth observation and geospatial applications. This project demonstrates an event-driven workflow for detecting water bodies in Sentinel-2 satellite imagery using Argo Events, Argo Workflows, and Calrissian. </p> <p>It utilizes a Redis stream to trigger workflows that process Sentinel-2 imagery and generate outputs using CWL (Common Workflow Language).</p>"},{"location":"#key-components","title":"Key Components","text":"<p>This setup integrates the following technologies and concepts:</p>"},{"location":"#argo-workflows","title":"Argo Workflows","text":"<ul> <li>Automates the execution of tasks using predefined workflow templates.</li> <li>Supports the execution of CWL workflows using Calrissian, a lightweight executor for CWL in Kubernetes.</li> <li>Includes two templates:</li> <li>CWL Execution Template: Executes general CWL workflows, such as preprocessing tasks.</li> <li>Water Bodies Detection Template: Encodes the algorithm for detecting water bodies in geospatial data.</li> </ul>"},{"location":"#argo-events","title":"Argo Events","text":"<ul> <li>Provides an event-driven architecture for triggering workflows.</li> <li>Uses a Jetstream Event Bus to handle event communication.</li> <li>Includes:</li> <li>Redis Event Source: Queries the STAC endpoint to generate simulated events.</li> <li>Event Sensor: Listens for Redis events and triggers the water bodies detection workflow.</li> </ul>"},{"location":"#redis-as-an-event-source","title":"Redis as an Event Source","text":"<ul> <li>Simulates events by querying the STAC endpoint.</li> <li>Acts as a lightweight, flexible mechanism to mimic real-time event streams.</li> <li>Ensures seamless integration with Argo Events via an event source configuration.</li> </ul>"},{"location":"#stac-endpoint","title":"STAC Endpoint","text":"<ul> <li>Serves as the primary data source, providing geospatial data in a standardized format.</li> <li>Enables the workflow to focus on processing relevant datasets for water bodies detection.</li> </ul>"},{"location":"#high-level-architecture","title":"High-Level Architecture","text":"<p>The system is designed to handle the following flow:</p> <ol> <li> <p>Event Generation:</p> </li> <li> <p>Redis queries the STAC endpoint and generates events containing metadata about geospatial assets (e.g., imagery of specific regions).</p> </li> <li> <p>Event Propagation:</p> </li> <li> <p>The Redis Event Source forwards events to the Jetstream Event Bus.</p> </li> <li> <p>Event Sensing and Workflow Triggering:</p> </li> <li> <p>The Event Sensor monitors the Jetstream Event Bus for relevant events.</p> </li> <li> <p>Upon detecting an event, the sensor triggers the execution of the water bodies detection workflow.</p> </li> <li> <p>Workflow Execution:</p> </li> <li> <p>The Argo Workflow templates process the event's input data using the CWL-based algorithm.</p> </li> <li>The water bodies detection results are stored or published for further use.</li> </ol>"},{"location":"#why-use-this-setup","title":"Why Use This Setup?","text":"<p>This setup showcases the power of combining event-driven paradigms with container-native workflows for scalable geospatial analysis. </p> <p>It is particularly suited for Earth observation and scientific workflows because:</p> <ul> <li>Scalability: Kubernetes ensures workflows can handle varying loads effectively.</li> <li>Modularity: Components can be easily reused or replaced for other applications.</li> <li>Automation: Events trigger workflows without manual intervention, enabling real-time processing.</li> </ul> <p>Through this resource, you'll learn to implement a cloud-native pipeline for water bodies detection, which can be extended to other geospatial or scientific applications.</p>"},{"location":"argo-events/","title":"Argo Events","text":""},{"location":"argo-events/#argo-events","title":"Argo Events","text":""},{"location":"argo-events/#introduction","title":"Introduction","text":"<p>Argo Events provides a Kubernetes-native event-driven automation framework. </p> <p>In this setup, it is used to trigger workflows based on events generated by Redis. </p> <p>This section explores the key components of Argo Events: the Jetstream event bus, the Redis event source, and the event sensor that drives the execution of the water bodies detection pipeline.</p>"},{"location":"argo-events/#key-features-of-argo-events","title":"Key Features of Argo Events","text":"<ul> <li>Event-Driven Execution: Automates workflows in response to real-time events.</li> <li>Scalable Architecture: Decouples event sources from workflow execution.</li> <li>Modularity: Supports multiple event sources, including HTTP, Redis, and more.</li> <li>Native Integration: Seamlessly integrates with Argo Workflows to trigger pipelines.</li> </ul>"},{"location":"argo-events/#components-of-argo-events","title":"Components of Argo Events","text":"<ol> <li>Event Bus</li> </ol> <p>The event bus serves as the messaging backbone, enabling communication between event sources and sensors. In this setup, a Jetstream event bus is used for reliable, high-performance event delivery.</p> <p>Why Jetstream?</p> <ul> <li>High throughput and reliability.</li> <li> <p>Built-in message persistence for fault tolerance.</p> </li> <li> <p>Event Source</p> </li> </ul> <p>The event source monitors Redis for changes and forwards relevant events to the event bus. </p> <p>Redis simulates incoming events by querying a STAC endpoint and publishing results.</p> <p>Key Features:</p> <ul> <li>Channel-Based Filtering: Listens to specific channels for relevant events.</li> <li> <p>Secure Connections: Supports secret-based password authentication.</p> </li> <li> <p>Event Sensor</p> </li> </ul> <p>The event sensor listens for events on the Jetstream event bus and triggers the execution of workflows when criteria are met.</p> <p>Key Features:</p> <ul> <li>Dependencies: Defines the event source and type to listen for.</li> <li>Trigger Templates: Configures the workflow to be executed upon an event.</li> </ul>"},{"location":"argo-events/#event-flow","title":"Event Flow","text":"<ol> <li>Event Generation:</li> </ol> <p>Redis generates events by querying a STAC endpoint for new geospatial data.</p> <p>Events are published to the stac-events channel.</p> <ol> <li>Event Source Handling:</li> </ol> <p>The Redis event source monitors the stac-events channel and forwards messages to the Jetstream event bus.</p> <ol> <li>Sensor Activation:</li> </ol> <p>The sensor listens to the Jetstream event bus.</p> <p>Upon receiving an event, it triggers the water-bodies-detection workflow.</p> <ol> <li>Workflow Execution:</li> </ol> <p>Argo Workflows orchestrates the pipeline to process geospatial data and detect water bodies.</p>"},{"location":"argo-events/#why-use-argo-events","title":"Why Use Argo Events?","text":"<ul> <li>Seamless Workflow Triggers: Effortlessly connects events with workflows.</li> <li>Flexibility: Supports various event sources, making it adaptable to different scenarios.</li> <li>Scalability: Can handle high event throughput with minimal latency.</li> <li>Kubernetes-Native: Fully integrates with Kubernetes for a unified ecosystem.</li> </ul>"},{"location":"argo-workflows/","title":"Workflow Orchestration with Argo Workflows","text":""},{"location":"argo-workflows/#introduction","title":"Introduction","text":"<p>Argo Workflows is a Kubernetes-native workflow orchestration tool that excels at automating complex pipelines.</p> <p>This section explains how Argo Workflows is utilized in this setup to execute the water bodies detection algorithm and related preprocessing tasks.</p> <p>It focuses on the two workflow templates at the core of this system:</p>"},{"location":"argo-workflows/#cwl-execution-template","title":"CWL Execution Template","text":""},{"location":"argo-workflows/#water-bodies-detection-template","title":"Water Bodies Detection Template","text":"<p>These templates form the backbone of the processing pipeline, enabling the execution of tasks based on events triggered by Argo Events.</p> <p>Key Features of Argo Workflows</p> <ul> <li>Container-Native: Each step of the workflow runs in its own container, ensuring isolation and scalability.</li> <li>Declarative Workflows: Defined in YAML, allowing easy customization and version control.</li> <li>Scalability: Leverages Kubernetes to run workflows efficiently across distributed resources.</li> <li>Integration: Supports external tools like Calrissian for CWL execution, making it ideal for Earth Observation Application Packages workflows.</li> </ul>"},{"location":"argo-workflows/#workflow-templates","title":"Workflow Templates","text":"<p>This setup uses two primary workflow templates, each designed for specific tasks:</p> <ol> <li>CWL Execution Template</li> </ol> <p>This template executes generic CWL workflows using Calrissian, a lightweight CWL runner optimized for Kubernetes.</p> <p>Purpose:</p> <p>To process general data preparation tasks or execute modular components of the detection pipeline.</p> <ol> <li>Water Bodies Detection Template</li> </ol> <p>This template implements the core water bodies detection algorithm. </p> <p>It processes geospatial data retrieved from the STAC endpoint and identifies water bodies using the defined logic.</p> <p>Purpose:</p> <p>To apply the detection algorithm to geospatial datasets, generating actionable results.</p>"},{"location":"argo-workflows/#workflow-execution-flow","title":"Workflow Execution Flow","text":"<p>1 Triggered by Events: Workflows are initiated when Argo Events detects an event matching the criteria.</p> <ol> <li>Data Preparation:</li> </ol> <p>The CWL Execution Template runs preprocessing steps, such as data transformation or tiling.</p> <ol> <li>Algorithm Execution:</li> </ol> <p>The Water Bodies Detection Template processes the prepared data and generates results.</p> <ol> <li>Result Handling:</li> </ol> <p>Output data, such as <code>GeoJSON</code> files, is stored or published for further analysis.</p>"},{"location":"argo-workflows/#why-use-argo-workflows","title":"Why Use Argo Workflows?","text":"<ul> <li>Ease of Use: Declarative <code>YAML</code> syntax simplifies workflow definition.</li> <li>Extensibility: Easily integrate custom containers and tools like Calrissian.</li> <li>Kubernetes-Native: Leverages Kubernetes' orchestration capabilities for resource efficiency.</li> <li>Event-Driven Compatibility: Works seamlessly with Argo Events for real-time pipeline automation.</li> </ul>"},{"location":"components/","title":"Architecture","text":""},{"location":"flow/","title":"Integrating Event-Driven Execution with Argo Workflows","text":""},{"location":"flow/#introduction","title":"Introduction","text":"<p>This page details the end-to-end integration and execution flow of the water bodies detection system. </p> <p>By combining Argo Workflowsand Argo Events, the system achieves seamless automation triggered by geospatial data events.</p>"},{"location":"flow/#integration-architecture","title":"Integration Architecture","text":"<p>The integration involves three main layers:</p> <ul> <li>Event Source Layer: Monitors Redis for events simulated by querying the STAC endpoint.</li> <li>Event Routing Layer: The Jetstream event bus routes events from the Redis source to the sensor.</li> <li>Workflow Execution Layer: Argo Workflows processes geospatial data and detects water bodies.</li> </ul>"},{"location":"flow/#end-to-end-execution-flow","title":"End-to-End Execution Flow","text":""},{"location":"flow/#step-1-event-generation","title":"Step 1: Event Generation","text":"<p>Description: Redis acts as an intermediary for event generation. Events are created by querying a STAC API endpoint for geospatial data.</p> <p>Details: * Queries can include filters such as specific time ranges or geographic areas. * Redis publishes events to the stac-events channel.</p>"},{"location":"flow/#step-2-event-source-monitoring","title":"Step 2: Event Source Monitoring","text":"<p>Description: The Redis event source listens for new messages on the stac-events channel.</p> <p>Details: * When a new event is detected, it forwards it to the Jetstream event bus. * Events include metadata such as STAC item IDs, collection details, and timestamps.</p>"},{"location":"flow/#step-3-event-routing","title":"Step 3: Event Routing","text":"<p>Description: The Jetstream event bus acts as a high-performance router for events.</p> <p>Details: * Ensures reliable delivery to the event sensor. * Supports scalable handling of multiple concurrent events.</p>"},{"location":"flow/#step-4-sensor-activation","title":"Step 4: Sensor Activation","text":"<p>Description: The event sensor listens to the Jetstream bus and triggers workflows.</p> <p>Details: * Matches events based on defined criteria (e.g., specific STAC item properties). * Passes event metadata to the triggered workflow as input parameters.</p>"},{"location":"flow/#step-5-workflow-execution","title":"Step 5: Workflow Execution","text":"<p>Description: The Argo Workflow executes the pipeline to process geospatial data.</p> <p>Details:</p> <ul> <li>The workflow includes two templates:</li> <li>Calrissian Template: Runs a CWL pipeline to pre-process data.</li> <li>Detection Template: Executes the water bodies detection algorithm.</li> <li>Outputs include a GeoTiff file with detected water bodies described as a STAC Item, logs, and diagnostic data.</li> </ul>"},{"location":"flow/#step-6-results-delivery","title":"Step 6: Results Delivery","text":"<p>Description: The workflow stores outputs in a predefined storage location and updates the STAC catalog with results.</p> <p>Details:</p> <ul> <li>Results are made accessible via STAC API endpoints.</li> <li>Users or downstream applications can retrieve the outputs for analysis.</li> </ul>"},{"location":"flow/#integration-diagram","title":"Integration Diagram","text":"<p>TODO (Insert a flowchart or diagram illustrating the flow: Redis \u2192 Event Source \u2192 Jetstream \u2192 Event Sensor \u2192 Workflow Execution \u2192 Result Storage)</p>"},{"location":"flow/#key-benefits-of-the-integration","title":"Key Benefits of the Integration","text":"<ul> <li>Automation: Fully automates the data processing pipeline from event generation to result delivery.</li> <li>Scalability: Supports high-throughput event handling and parallel workflow execution.</li> <li>Modularity: Easy to extend with additional event sources or processing workflows.</li> <li>Real-Time Processing: Responds to geospatial data changes in near real-time.</li> </ul>"},{"location":"flow/#how-to-test-the-system","title":"How to Test the System","text":"<ol> <li>Simulate Events: Publish STAC query results to the Redis channel manually or via automation.</li> <li>Monitor Workflow Execution: Use the Argo Workflows UI to track pipeline progress.</li> <li>Validate Outputs: Verify that the workflow generates correct water bodies detection results and updates the STAC catalog.</li> </ol>"},{"location":"hands-on/","title":"Hands-on","text":""},{"location":"hands-on/#deployments","title":"Deployments","text":"<p>Open event-driven-with-argo to learn how to deploy deferent components on a cluster.</p>"},{"location":"hands-on/#redis-subscriber-implementation","title":"Redis' Subscriber Implementation:","text":"<p>Open notebook \"sub.ipynb\" to run a hands-on exercise on Redis' Subscriber. Note that, it is essential to run subscriber before publisher to ensure no events are missed.</p>"},{"location":"hands-on/#redis-publisher-implementation","title":"Redis' Publisher Implementation","text":"<p>Open notebook \"pub.ipynb\" to run a hands-on exercise on Redis' Publisher.</p> <p>Notice: You must monitor both running notebooks to analyse the process of event handeling.</p>"},{"location":"insights/","title":"Lessons Learned from Building an Event-Driven Geospatial Data Pipeline","text":""},{"location":"insights/#introduction","title":"Introduction","text":"<p>This page highlights the technical challenges, design decisions, and key insights gained while developing the event-driven geospatial data pipeline. </p> <p>It also includes recommendations for future improvements and practical advice for replicating or extending the setup.</p>"},{"location":"insights/#key-technical-insights","title":"Key Technical Insights","text":""},{"location":"insights/#event-source-configuration","title":"Event Source Configuration","text":"<p>Challenge: Configuring Redis as a reliable event source for Argo Events.</p> <p>Insight: * Redis' Pub/Sub feature proved effective but required careful setup to ensure scalability. * The use of clear naming conventions for Redis channels (e.g., stac-events) simplified debugging.</p>"},{"location":"insights/#jetstream-event-bus","title":"Jetstream Event Bus","text":"<p>Challenge: Ensuring reliable and performant event routing.</p> <p>Insight: * Jetstream\u2019s integration with Argo Events offers a robust event-driven architecture. * It is crucial to define clear message schemas for event payloads to maintain consistency.</p>"},{"location":"insights/#workflow-scalability","title":"Workflow Scalability","text":"<p>Challenge: Managing workflows triggered by multiple simultaneous events.</p> <p>Insight: * Argo Workflows scales effectively but requires resource limits and quotas to prevent cluster overload. * Using parameterized templates enabled workflows to process diverse input data dynamically.</p>"},{"location":"insights/#design-decisions","title":"Design Decisions","text":""},{"location":"insights/#separation-of-concerns","title":"Separation of Concerns","text":"<p>Decision: Use Argo Events exclusively for orchestration and delegate processing to workflows.</p> <p>Outcome: * Simplified troubleshooting by isolating event-handling logic from data processing logic.</p>"},{"location":"insights/#modular-workflow-templates","title":"Modular Workflow Templates","text":"<p>Decision: Separate the CWL execution and water bodies detection into distinct workflow templates.</p> <p>Outcome: * Enhanced reusability for other geospatial pipelines requiring similar preprocessing steps.</p>"},{"location":"insights/#stac-integration","title":"STAC Integration","text":"<p>Decision: Leverage the STAC API for querying and storing geospatial data.</p> <p>Outcome: * Improved interoperability with other geospatial tools and standards.</p>"},{"location":"insights/#challenges-and-solutions","title":"Challenges and Solutions","text":""},{"location":"insights/#handling-event-storms","title":"Handling Event Storms","text":"<p>Challenge: Preventing resource exhaustion during high-frequency event generation.</p> <p>Solution: * Introduced rate-limiting on the Redis event source and implemented a backoff mechanism.</p>"},{"location":"insights/#debugging-workflow-failures","title":"Debugging Workflow Failures","text":"<p>Challenge: Diagnosing failures in complex workflows with multiple steps.</p> <p>Solution: * Enabled Argo Workflows\u2019 artifact repository to store intermediate outputs and logs for analysis.</p>"},{"location":"insights/#maintaining-consistency-in-results","title":"Maintaining Consistency in Results","text":"<p>Challenge: Ensuring consistent outputs across diverse input datasets.</p> <p>Solution: * Developed unit tests for the CWL pipeline and water bodies detection algorithm. * Validated results against a benchmark dataset during development.</p>"},{"location":"insights/#conclusion","title":"Conclusion","text":"<p>Using Kubernetes-native tools like Argo Workflows and Argo Events, this setup demonstrates how to create a real-time processing system that aligns with modern cloud-native practices.</p>"},{"location":"sequence/","title":"Sequence Diagrams","text":""},{"location":"sequence/#deployment","title":"Deployment","text":""},{"location":"sequence/#ingestion","title":"Ingestion","text":""},{"location":"topics/","title":"Topics","text":""}]}